{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"StyleGAN_Encoder_Tutorial.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"_RuGwMk24EWI","colab_type":"text"},"source":["This is pbaylies' StyleGAN Encoder and tutorial, ported to StyleGAN 2.\n","  \n"," * Note: make sure you're using the GPU runtime for this - see Runtime -> Change Runtime Type in Google Colab"]},{"cell_type":"code","metadata":{"id":"NuM_vG4239Wm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":259},"outputId":"af05e84b-19b2-4a7e-b67d-ec9f2a6b2450","executionInfo":{"status":"ok","timestamp":1577385348127,"user_tz":-60,"elapsed":4996,"user":{"displayName":"Robert Luxemburg","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDeQ9LmepNejewRj8Q_UQm6EEj6WBV1zFD3LRtOyw=s64","userId":"13783897405164658633"}}},"source":["!pip install --force tqdm==4.24.0\n","%tensorflow_version 1.x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5D6umtIoe_rb","colab_type":"text"},"source":["First, clone the repo:"]},{"cell_type":"code","metadata":{"id":"JAy5VqXW7xRI","colab_type":"code","outputId":"09817c39-4c17-4724-8712-e2b3e3990faf","colab":{"base_uri":"https://localhost:8080/","height":153},"executionInfo":{"status":"ok","timestamp":1577385361838,"user_tz":-60,"elapsed":4508,"user":{"displayName":"Robert Luxemburg","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDeQ9LmepNejewRj8Q_UQm6EEj6WBV1zFD3LRtOyw=s64","userId":"13783897405164658633"}}},"source":["!git clone https://github.com/PaterAeneas/stylegan2encoder\n","%cd stylegan2encoder\n","!curl -O https://raw.githubusercontent.com/pbaylies/stylegan-encoder/master/config.py "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VSVKYuhteMgp","colab_type":"text"},"source":["TODO: Optionally, try training a ResNet of your own if you like; this could take a while. Remember, there's a pre-trained model linked in the repo that works with the FFHQ faces StyleGAN model)"]},{"cell_type":"code","metadata":{"id":"MrtxsLef8hQ9","colab_type":"code","colab":{}},"source":["#!python train_resnet.py --help\n","#!python train_resnet.py --test_size 256 --batch_size 1024 --loop 1 --max_patience 1\n","\n","# Upload finetuned_resnet.h5 to colab storage\n","# !mv /content/finetuned_resnet.h5 /content/stylegan2encoder/data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NkEDqfuJHk8V","colab_type":"text"},"source":["Next, let's get some test images to work with... These are from Pexels, which has free stock photos."]},{"cell_type":"code","metadata":{"id":"21v8O2jL-oMq","colab_type":"code","outputId":"66711293-c884-4a5c-8751-b3d61f7db5a0","colab":{"base_uri":"https://localhost:8080/","height":445},"executionInfo":{"status":"ok","timestamp":1577385519712,"user_tz":-60,"elapsed":5778,"user":{"displayName":"Robert Luxemburg","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDeQ9LmepNejewRj8Q_UQm6EEj6WBV1zFD3LRtOyw=s64","userId":"13783897405164658633"}}},"source":["!mkdir aligned_images raw_images\n","!wget -O raw_images/stock_photo1.jpg 'https://images.pexels.com/photos/614810/pexels-photo-614810.jpeg?auto=compress&cs=tinysrgb&dpr=3&h=1536&w=1536'\n","!wget -O raw_images/stock_photo2.jpg 'https://images.pexels.com/photos/1445467/pexels-photo-1445467.jpeg?auto=compress&cs=tinysrgb&dpr=3&h=1536&w=1536'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t0hakluVJXvO","colab_type":"code","outputId":"6210a33c-e475-4339-a8fb-353cf3347ee3","colab":{"base_uri":"https://localhost:8080/","height":405},"executionInfo":{"status":"ok","timestamp":1577385531457,"user_tz":-60,"elapsed":2590,"user":{"displayName":"Robert Luxemburg","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDeQ9LmepNejewRj8Q_UQm6EEj6WBV1zFD3LRtOyw=s64","userId":"13783897405164658633"}}},"source":["import PIL.Image\n","img1 = PIL.Image.open('raw_images/stock_photo1.jpg')\n","wpercent = (256/float(img1.size[0]))\n","hsize = int((float(img1.size[1])*float(wpercent)))\n","img1 = img1.resize((256,hsize), PIL.Image.LANCZOS)\n","img2 = PIL.Image.open('raw_images/stock_photo2.jpg')\n","wpercent = (256/float(img2.size[0]))\n","hsize = int((float(img2.size[1])*float(wpercent)))\n","img2 = img2.resize((256,hsize), PIL.Image.LANCZOS)\n","display(img1,img2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4MM8TtbwK5w8","colab_type":"text"},"source":["Now we need to get just the faces, cropped and aligned... fortunately, there's already a script for this!"]},{"cell_type":"code","metadata":{"id":"M_Er59x8Mt54","colab_type":"code","outputId":"4ab57a54-8784-423e-e2dc-b2c493641eda","colab":{"base_uri":"https://localhost:8080/","height":972},"executionInfo":{"status":"ok","timestamp":1577387375368,"user_tz":-60,"elapsed":5986,"user":{"displayName":"Robert Luxemburg","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDeQ9LmepNejewRj8Q_UQm6EEj6WBV1zFD3LRtOyw=s64","userId":"13783897405164658633"}}},"source":["!python align_images.py raw_images/ aligned_images/\n","\n","display(PIL.Image.open('aligned_images/stock_photo1_01.png').resize((256,256)))\n","display(PIL.Image.open('aligned_images/stock_photo2_01.png').resize((256,256)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s0mtyFkhHRMU","colab_type":"text"},"source":["Finally, let's try encoding some images into a latent representation! That's what you came here for, right? Let's just do a quick encoding, and see how we do..."]},{"cell_type":"markdown","metadata":{"id":"1fiBJKnbMmM5","colab_type":"text"},"source":["Ok, let's see how we did! Note the paths above, the generated image is in the generated_images folder, the latent representation is in the latent_representations folder, and since we generated videos of the training process, by default those are in the videos folder."]},{"cell_type":"markdown","metadata":{"id":"hVs2hVEDOQf1","colab_type":"text"},"source":["Great job, everybody, nailed it! ... wait, what's that you say, the second image doesn't quite look the same? Hmm... let's try to do a better job, shall we?"]},{"cell_type":"code","metadata":{"id":"JAiOavJXoI9k","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"c52515d5-f1a5-4ed1-d064-5862e577a9f5","executionInfo":{"status":"ok","timestamp":1577388298043,"user_tz":-60,"elapsed":282633,"user":{"displayName":"Robert Luxemburg","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDeQ9LmepNejewRj8Q_UQm6EEj6WBV1zFD3LRtOyw=s64","userId":"13783897405164658633"}}},"source":["!python encode_images.py aligned_images/ generated_images/ latent_representations/ \\\n","    # --lr=0.25 --iterations=1000 --use_l1_penalty=0.5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ahVIrJrDOdD-","colab_type":"code","outputId":"7099a303-e2a0-49f6-8820-54fc3c57ff3b","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1577388371327,"user_tz":-60,"elapsed":4686,"user":{"displayName":"Robert Luxemburg","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDeQ9LmepNejewRj8Q_UQm6EEj6WBV1zFD3LRtOyw=s64","userId":"13783897405164658633"}}},"source":["display(PIL.Image.open('generated_images/stock_photo1_01.png').resize((512, 512)))\n","display(PIL.Image.open('generated_images/stock_photo1_01.png').resize((512, 512)))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WnDiLR0yEpLh","colab_type":"text"},"source":["When in doubt, mess with the parameters... here, we ~lowered the l1 penalty on the dlatents, to allow for greater variation outside of what StyleGAN knows well, and we've lowered the learning rate and~ raised the number of iterations~, to allow for more gradual changes~. See more options below:"]},{"cell_type":"code","metadata":{"id":"NWTzjcfmE9Jv","colab_type":"code","outputId":"77792eae-aac7-4bc0-c868-cb098f2469be","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1577386794867,"user_tz":-60,"elapsed":4590,"user":{"displayName":"Robert Luxemburg","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDeQ9LmepNejewRj8Q_UQm6EEj6WBV1zFD3LRtOyw=s64","userId":"13783897405164658633"}}},"source":["!python encode_images.py --help"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dPOfaK9AQ-bT","colab_type":"text"},"source":["## ~ Here Be Dragons ~\n","\n","Ok, that's nice, but what to do with the latents? Well, I'm glad you asked, let's see what we can do with the two latent representations we now have to rub together..."]},{"cell_type":"code","metadata":{"id":"yRxJu664Tp5R","colab_type":"code","outputId":"ca5ef3f6-2834-4a9a-c10c-c8db8b1cc5d8","colab":{"base_uri":"https://localhost:8080/","height":600}},"source":["import math\n","import pickle\n","import PIL.Image\n","import numpy as np\n","import config\n","import dnnlib\n","import dnnlib.tflib as tflib\n","from encoder.generator_model import Generator\n","\n","URL_FFHQ = 'gdrive:networks/stylegan2-ffhq-config-f.pkl'\n","tflib.init_tf()\n","with dnnlib.util.open_url(URL_FFHQ) as f:\n","    generator_network, discriminator_network, Gs_network = pickle.load(f)\n","\n","generator = Generator(Gs_network, batch_size=1, randomize_noise=False)\n","\n","model_res = 1024\n","model_scale = int(2*(math.log(model_res,2)-1))\n","\n","def generate_image(latent_vector):\n","    latent_vector = latent_vector.reshape((1, 18, 512))\n","    generator.set_dlatents(latent_vector)\n","    img_array = generator.generate_images()[0]\n","    img = PIL.Image.fromarray(img_array, 'RGB')\n","    return img.resize((256, 256))\n","\n","def move_and_show(latent_vector, direction, coeffs):\n","    fig,ax = plt.subplots(1, len(coeffs), figsize=(15, 10), dpi=80)\n","    for i, coeff in enumerate(coeffs):\n","        new_latent_vector = latent_vector.copy()\n","        new_latent_vector[:8] = (latent_vector + coeff*direction)[:8]\n","        ax[i].imshow(generate_image(new_latent_vector))\n","        ax[i].set_title('Coeff: %0.1f' % coeff)\n","    [x.axis('off') for x in ax]\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RkRvyBzJSkSE","colab_type":"code","outputId":"4e4ac2c2-6fa3-4a29-8376-0efc5caf1868","colab":{"base_uri":"https://localhost:8080/","height":1041}},"source":["stock_photo1 = np.load('/content/stylegan2encoder/latent_representations/stock_photo2_01.npy')\n","stock_photo2 = np.load('/content/stylegan2encoder/latent_representations/stock_photo1_01.npy')\n","# Upload latent directions\n","# !mv /content/*.npy /content/stylegan2encoder/ffhq_dataset\n","# gender_direction = np.load('/content/stylegan2encoder/ffhq_dataset/gender.npy')\n","# age_direction = np.load('/content/stylegan2encoder/ffhq_dataset/age.npy')\n","# smile_direction = np.load('/content/stylegan2encoder/ffhq_dataset/smile.npy')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qleOmCJ_7m0X","colab_type":"text"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["move_and_show(stock_photo1, gender_direction, [-2, 0, 2])\n","move_and_show(stock_photo2, gender_direction, [-2, 0, 2])"]}]}